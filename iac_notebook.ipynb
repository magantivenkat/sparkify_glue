{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iac: Create S3 Bucket and Launch Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import configparser\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('airflow/config/aws.cfg'))\n",
    "\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "BUCKET = config.get('AWS','BUCKET')\n",
    "REGION = config.get('AWS', 'REGION')\n",
    "CRAWLER = config.get('AWS', 'CRAWLER')\n",
    "\n",
    "DL_DB_USER = config.get('DL', 'DL_DB_USER')\n",
    "DL_DB_PASSWORD = config.get('DL', 'DL_DB_PASSWORD')\n",
    "DL_DB = config.get('DL', 'DL_DB')\n",
    "DL_IAM_ROLE_NAME = config.get('DL', 'DL_IAM_ROLE_NAME')\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DL_DB\", \"DL_DB_USER\", \"DL_DB_PASSWORD\", \"DL_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DL_DB, DL_DB_USER, DL_DB_PASSWORD, DL_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instaniate AWS Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET,\n",
    "                    region_name=REGION)\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                   aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET,\n",
    "                   region_name=REGION)\n",
    "\n",
    "glue = boto3.client(\n",
    "    service_name='glue',\n",
    "    aws_access_key_id=config.get('AWS', 'KEY'),\n",
    "    aws_secret_access_key=config.get('AWS', 'SECRET'),\n",
    "    region_name=config.get('AWS', 'REGION'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AWS Glue Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Data Type issues in staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "# get glue inferred schema\n",
    "medicare_dynamicframe = glueContext.create_dynamic_frame.from_catalog(\n",
    "       database = \"payments\",\n",
    "       table_name = \"medicare\")\n",
    "medicare_dynamicframe.printSchema()\n",
    "\n",
    "medicare_res = medicare_dynamicframe.resolveChoice(specs = [('provider id','cast:long')])\n",
    "medicare_res.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AWS S3 Sample Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip data.zip to create sample_data/ (35 MB)\n",
    "! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create s3 bucket\n",
    "try:\n",
    "    s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={'LocationConstraint': REGION})\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# local path to sample_data\n",
    "local_path = 'data/'\n",
    "\n",
    "#for file in local_path, add to s3 bucket\n",
    "file_count = 0\n",
    "print_every = 100\n",
    "for root,dirs,files in os.walk(local_path):\n",
    "    for file in files:\n",
    "        if file == '.DS_Store':\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        local_file_path = os.path.join(root,file)\n",
    "        bucket_file_path = os.path.join(root.replace(local_path,'',1),file)\n",
    "        s3.Object(BUCKET, bucket_file_path).put(Body=open(local_file_path, 'rb'))\n",
    "        \n",
    "        file_count += 1\n",
    "        if file_count % print_every == 0:\n",
    "            print('Files Uploaded: {}'.format(file_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Attach IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the role, make sure the AWS user defined in the aws.cfg has permission to create roles and attach policies or has administrative access. For the sake of simplicity, using a user with administrative access would be ideal. The cell below is for reference given a scenario where explicit role policies are needed to be given to specific user(s). The admin user would have to enter their key and secret values in the cell below. The cell will overide the admin variable with an empty value after the policies are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "admin = boto3.client('iam',\n",
    "                   aws_access_key_id='',\n",
    "                   aws_secret_access_key='',\n",
    "                   region_name=REGION\n",
    "                  )\n",
    "                  \n",
    "user_role_policy = json.dumps(\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:AttachRolePolicy\",\n",
    "                \"iam:CreateRole\",\n",
    "                \"iam:PutRolePolicy\",\n",
    "                \"iam:GetRole\",\n",
    "                \"iam:DetachRolePolicy\",\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "             \"Resource\": f\"arn:aws:s3:::{BUCKET}\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "# create user policy\n",
    "try:\n",
    "    user_role_arn = admin.create_policy(\n",
    "    PolicyName='RolePolicy',\n",
    "    Path='/',\n",
    "    PolicyDocument=user_role_policy,\n",
    "    Description='Allows user to manage roles')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "#attach user policy to user\n",
    "try:\n",
    "    response = admin.attach_user_policy(\n",
    "    UserName=DL_DB_USER,\n",
    "    PolicyArn=user_role_arn['Policy']['Arn'])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# overite admin variable for security purposes\n",
    "admin = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crawler_s3_access_policy = json.dumps(\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": \n",
    "        [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \n",
    "                [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:PutObject\"\n",
    "                ],\n",
    "                \"Resource\": \n",
    "                    [\n",
    "                    f\"arn:aws:s3:::{BUCKET}*\"\n",
    "                    ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "    \n",
    "role_trust_policy = json.dumps(\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"glue.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# create IAM role\n",
    "print(\"Creating Role\")\n",
    "try:\n",
    "    role = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DL_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Glue Crawler to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=role_trust_policy\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Attaching Policy\")\n",
    "try:\n",
    "    sparkify_policy = iam.create_policy(\n",
    "        PolicyName='AWSGlue-Sparkify',\n",
    "        Path='/',\n",
    "        PolicyDocument=crawler_s3_access_policy,\n",
    "        Description='Allows user to access sparkify bucket')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "response = iam.attach_role_policy(\n",
    "    RoleName=DL_IAM_ROLE_NAME,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole')\n",
    "\n",
    "response = iam.attach_role_policy(\n",
    "    RoleName=DL_IAM_ROLE_NAME,\n",
    "    PolicyArn=sparkify_policy['Policy']['Arn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    crawler = glue.get_crawler(Name=CRAWLER)\n",
    "# except EntityNotFoundException as e:\n",
    "except Exception as e:\n",
    "    print(f'\"{CRAWLER}\" crawler is not found')\n",
    "    print(f'Creating \"{CRAWLER}\" crawler')\n",
    "    response = glue.create_crawler(\n",
    "        Name=CRAWLER,\n",
    "        Role=DL_IAM_ROLE_NAME,\n",
    "        DatabaseName='sparkify',\n",
    "        Description=\"Crawler for generated schema\",\n",
    "        Targets={\n",
    "            'S3Targets': [\n",
    "                {\n",
    "                    'Path': f's3://{BUCKET}/song_data',\n",
    "                    'Exclusions': []\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "        SchemaChangePolicy={\n",
    "            'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "            'DeleteBehavior': 'DELETE_FROM_DATABASE'\n",
    "        }\n",
    "        #,Configuration='{ \"Version\": 1.0, \"CrawlerOutput\": { \"Partitions\": { \"AddOrUpdateBehavior\": \"InheritFromTable\" } } }'\n",
    "    )\n",
    "\n",
    "    crawler = glue.get_crawler(Name=CRAWLER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glue.start_crawler(Name=CRAWLER)\n",
    "\n",
    "crawler = glue.get_crawler(Name=CRAWLER)\n",
    "crawler_state = crawler['Crawler']['State']\n",
    "sleep_secs = 100\n",
    "\n",
    "while crawler_state != 'READY':\n",
    "    time.sleep(sleep_secs)\n",
    "    metrics = glue.get_crawler_metrics(CrawlerNameList=[CRAWLER])['CrawlerMetricsList'][0]\n",
    "    if metrics['StillEstimating'] == True:\n",
    "        pass\n",
    "    else:\n",
    "        time_left = int(metrics['TimeLeftSeconds'])\n",
    "        if time_left > 0:\n",
    "            print('Estimated Time Left: ', time_left)\n",
    "            sleep_secs = time_left\n",
    "        else:\n",
    "            print('Crawler should finish soon')\n",
    "            crawler = glue.get_crawler(Name=CRAWLER)\n",
    "    # refresh crawler state\n",
    "    crawler = glue.get_crawler(Name=CRAWLER)\n",
    "    crawler_state = crawler['Crawler']['State']\n",
    "\n",
    "metrics = glue.get_crawler_metrics(CrawlerNameList=['sparkify_crawler'])['CrawlerMetricsList'][0]\n",
    "print('\\nTable Metrics')\n",
    "print('Number of Tables Created: ', metrics['TablesCreated'])\n",
    "print('Number of Tables Updated: ', metrics['TablesUpdated'])\n",
    "print('Number of Tables Deleted: ', metrics['TablesDeleted'])\n",
    "\n",
    "print('Crawler is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Postgres Database for Airflow Backend\n",
    "\n",
    "Task parrelization isn't available with the default sqlite backend and sequential executor. A postgres database for the airflow backend will allow the local executor to be used for task parrelization.\n",
    "\n",
    "Instructions:\n",
    "- [Download Postgres UI App](https://www.postgresql.org/download/)\n",
    "- Within Postgres query editor or psql terminal, run: CREATE DATABASE database_name;\n",
    "- If you created the database with another user other than the default postgres user, add username to POSTGRES_USER below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment cell below to install sql magic\n",
    "# ! pip install ipython-sql\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER = \n",
    "POSTGRES_PASSWORD = \n",
    "POSTGRES_HOST = 'postgres'\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_DB = \n",
    "\n",
    "postgres_conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB)\n",
    "\n",
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read_file(open('airflow/config/airflow.cfg'))\n",
    "\n",
    "#update to postgres string\n",
    "config.set('core','sql_alchemy_conn', postgres_conn_string)\n",
    "#write changes to config file\n",
    "with open('airflow/config/airflow.cfg', 'w') as configfile:\n",
    "    config.write(configfile)\n",
    "    configfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('airflow': conda)",
   "language": "python",
   "name": "python38264bitairflowconda241aa32da08d477cb1d76686edce0c4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}